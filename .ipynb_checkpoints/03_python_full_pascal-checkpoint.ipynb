{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "\n",
    "Partly based on material of Elizaveta Semenova, Applied Machine Learning Days 2020, Bayesian Inference: embracing uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Monte Carlo Methods?\n",
    "\n",
    "Monte Carlo Methods are a class of algorithms that infer quantities by using random number generators.\n",
    "\n",
    "Monte Carlo Methods are probabilistic and not deterministic. This is, Monte Carlo Methods usually don't yield the same excat results when being repeating. They are statistical methods, which estimate quantities - or parameters - rather than determining them.\n",
    "\n",
    "In this Notebook, two examples of Monte Carlo Methods are discussed\n",
    "\n",
    " * Monte Carlo Integration\n",
    "   * $\\int_{0}^{1}e^{x}dx$\n",
    " * Parameter Estimation with Bayesian Inference\n",
    "   * Coin Toss - Estimating Head's Probability\n",
    "     * Analytically\n",
    "     * Metropollis-Hastings Random Walk Algorithm\n",
    "       * Manually\n",
    "       * PyMC\n",
    "   * Normal Distribution\n",
    "   * Linear Regression\n",
    "   * Binomial Likelihood\n",
    "   * Probabilistical Graphical Models\n",
    " \n",
    "\n",
    "\n",
    "## Monte Carlo Integration\n",
    " \n",
    "Monte Carlo Integration is one form of Numerical Integration. Numerical Integration calculates integrals $$ \\int_{a}^{b}f(x)dx$$ of Riemann-integrable functions computationally instead of determining them analytically. This is useful because finding closed-form solutions for integrals is often difficult. Numerical Integration generally computes integrals by approximating the function, which is to be integrated, by a step function. Monte Carlo Integration, on the other hand, estimates this integral by estimating the fraction of random points that fall below $f(x)$. Monte Carlo Integration scales better to integration in high-dimensional spaces compared to Numerical Integration that is based on a step function aproach, which suffers heavily from the curse of dimensionality.\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Determine the integral $\\int_0^1 e^x dx$.\n",
    "\n",
    "Analytically, this can be calculated by $e^1 - e^0 = e - 1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc3 as mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Integral Analytically\n",
    "np.exp(1) - np.exp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Integration - Visualization\n",
    "\n",
    "no_points = 1000\n",
    "\n",
    "x = np.linspace(0, 1, no_points)\n",
    "plt.plot(x, np.exp(x));\n",
    "pts = np.random.uniform(0,1,(no_points, 2))\n",
    "pts[:, 1] *= np.e\n",
    "\n",
    "cols = ['red'] * no_points\n",
    "for i in range(no_points):\n",
    "    if pts[i,1] > np.exp(pts[i,0]):     # acceptance / rejection step\n",
    "        cols[i] = 'steelblue'\n",
    "    \n",
    "plt.scatter(pts[:, 0], pts[:, 1], c = cols)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0, np.e]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Integration - Estimation\n",
    "\n",
    "for n in 10**np.array([1,2,3,4,5,6,7]):\n",
    "    pts = np.random.uniform(0, 1, (n, 2))\n",
    "    pts[:, 1] *= np.e\n",
    "    count = np.sum(pts[:, 1] < np.exp(pts[:, 0]))\n",
    "    volume = np.e * 1 # volume of region\n",
    "    sol = (volume * count)/n    \n",
    "    print(f'{n} {sol}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Parameter Estimation in Bayesian Inference\n",
    "\n",
    "In this second use case of Monte Carlo Methods, we we want to use Monte Carlo Methos to do staticial inference. In statistical inference we use data to infer properties of an underlying probability distribution. Or in other words, to estimate the parameters $\\theta \\in \\Theta$ of a statistical model.\n",
    "\n",
    "### Statistical Inference\n",
    "\n",
    "Statistical inference is a concept which is independent of Monte Carlo Methods. Monte Carlo Methods are rather one way to do statistical inference. Statistical inference can also be done by determining estimates of parameters of interest analytically, e.g. by pencil and paper. Determining parameter estimates analytically, however, can be difficult or impracticable because of complicated probability distributions or non-differentiability of probability of likelihood function wrt to the parameters of interest. In those cases, Monte Carlo Method offer an alternative to infer properties of an underlying propability distribution by using random sampling.\n",
    "\n",
    "Statistical inference can be done using different approaches. Most common are the frequentist approach and the Bayesian approach. These two approaches differ in how they use probability to express uncertainty in their respective statistical models. \n",
    "\n",
    "#### Frequentist Approach\n",
    "\n",
    "In the frequentist's approach to probability the parameters of a statistical model are fixed. We assume that there is only one \"true\" parameter combination that characterizes the underlying probability distribution. Our task is to find it. We estimate the \"true\" parameter combination by finding that parameter combination that explains the data with the highest possible likelihood. Finding and calculating this \"most plausible\" parameter combination is known as Maximum Likelihood Estimation (MLE). For MLE, we use only current data and no explicit prior belief about the parameters. In the frequentist's sense, probability is interpreted as the **frequency** of events.   \n",
    "\n",
    "\n",
    "#### Bayesian Approach\n",
    "\n",
    "In the Bayesian approach, on the other hand, probability represents **one's uncertainty**. We don't assume that the parameters describing underlying probability distributions are fixed. We rather assume that these parameters are non-deterministic - or at least that they present themselfes to us observers as non-deterministic. In this sense, we usually not only derive and calculate point estimates of parameters of interest but we rather infer whole probability distributions over those parameters describing and quantifying the uncertainty we associate with that parameters. Calculating those probability distributions, however, is generally more complicated than calculating point estimates in the MLE setting. This is, because for calculating probability distributions over paramters we need Bayes rule. \n",
    "\n",
    "Bayes rule follows from the definition of conditional probability\n",
    "\n",
    "$$ P(A | B) = \\frac{P(A, B)}{P(B)}$$\n",
    "\n",
    "$$ P(B | A) = \\frac{P(A, B)}{P(A)}$$\n",
    "\n",
    "And rearanging\n",
    "\n",
    "$$ P(A | B) = \\frac{P(B | A) P(A)}{P(B)}$$\n",
    "\n",
    "where $A \\in ð’œ$, $B \\in â„¬$ and $ð’œ \\times\tâ„¬$ is the product of measurable spaces ð’œ and â„¬. The same relationship is also true for probability density functions, i.e. \n",
    "\n",
    "\n",
    "$$ p(x | y) = \\frac{p(y | x) p(x)}{p(y)}$$\n",
    "\n",
    "In Bayesian approach to probability we usually have a random sample $x = x_{i=1, ...n}$ drawn from a random variable $X_{\\theta}$ whose underlying probability distribution is parametrized by $\\theta \\in \\Theta$. In Bayesian statistics, in contrast to the frequentist's approach to probability, we assume a probability distribution over $\\Theta$ as well. In practise, we usually want to find a point estimate $\\hat{\\theta}$ for the unknown model parameter and its distribution given the data $x$. We can calculate that distribution using Bayes' rule as follows\n",
    "\n",
    "$$p(\\theta | x) =  \\frac{p(x | \\theta) p(\\theta)}{p(x)}$$\n",
    "\n",
    "The right-hand side of this equation consists of three parts. \n",
    "\n",
    " * $p(x | \\theta)$ represents the probability of the data under the model $\\theta$. It has a similar form as the likelihood function in the frequentist's MLE setting $p(x; \\theta)$ and is thus called **likelihood**\n",
    " * $p(x)$ is the probability of observing the data $x$ and is called the **data** term\n",
    " * $p(\\theta)$ is the probability of the parameter $\\theta$ and is called the **prior**\n",
    " \n",
    "The left-hand sind of the equation above, $p(\\theta | x)$, is called the **posterior**. \n",
    "\n",
    "Since the data term $p(x)$ does not depend on $\\theta$ it is often dropped when computing the posterior. This makes life easier since this term can be hard to compute. To compute the marginal probability $p(x)$ we would need to integrate out the parameter $\\theta$ like $p(x) = \\int_{\\theta}p(x|\\theta)d\\theta$. So it is often enough to compute\n",
    "\n",
    "$$p(\\theta | x) \\propto p(x | \\theta) p(\\theta)$$\n",
    "\n",
    "In case we really need $p(x | \\theta)$, we can understand and calculate $p(x)$ as a normalization constant such that $\\int_{\\theta}p(x | \\theta)d\\theta$ = 1. This is rather easy when we approximate $p(\\theta | x)$ with Monte Carlo Methods.\n",
    "\n",
    "Parameter estimation using Bayes' approach through calculating the posterior $p(\\theta | x)$ requires/allows us thus to use both \n",
    " * current data $p(x | \\theta)$\n",
    " * prior knowledge and/or our belief about parameters $p(\\theta)$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
